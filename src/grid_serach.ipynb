{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage import feature, io, color\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dir = \"../data/test\"\n",
    "# test_data, test_labels = load_raw_data(test_dir)\n",
    "\n",
    "train_dir = \"../data/train\"\n",
    "train_data, train_labels = load_raw_data(train_dir)\n",
    "\n",
    "bins = [8, 16, 32, 64, 128, 256]\n",
    "sizes = [32, 64, 128, 250]  # default image_size = 250x250 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *SVM*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n",
      "Eplased time: 451.6034927368164\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "start = time.time()\n",
    "for bin in bins:\n",
    "    for img_size in sizes:\n",
    "        svm = SVC()\n",
    "\n",
    "        train_data_ = [preprocess2(x, img_size, bin) for x in train_data]\n",
    "\n",
    "        param_grid = {\n",
    "            'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "            'kernel': ['linear', 'rbf', 'poly', 'sigmoid'], \n",
    "            'gamma': [0.1, 0.5, 1, 5, 10] \n",
    "            # 'gamma' : [\"scale\", \"auto\"],\n",
    "        }       \n",
    "\n",
    "        # GridSearchCV\n",
    "        grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "        # Huấn luyện\n",
    "        grid_search.fit(train_data_, train_labels)\n",
    "        results.append({\n",
    "            \"bin\": bin,\n",
    "            \"img_size\": img_size,\n",
    "            \"model\": grid_search           \n",
    "        }\n",
    "        )\n",
    "\n",
    "print(\"DONE\")\n",
    "elapsed = time.time() - start\n",
    "print(f\"Eplased time: {elapsed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Overall Model:\n",
      "img_size: 64\n",
      "bin: 32\n",
      "Best Parameters: {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "{'img_size': 64, 'bin': 32, 'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./checkpoint/svm_best_model.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = None\n",
    "best_score = -float('inf')  # Đặt giá trị thấp nhất ban đầu để so sánh\n",
    "\n",
    "# Lặp qua tất cả các kết quả \"trong results\n",
    "for grid_search in results:\n",
    "    if grid_search[\"model\"].best_score_ > best_score:\n",
    "        best_score = grid_search[\"model\"].best_score_\n",
    "        best_model = grid_search[\"model\"].best_estimator_\n",
    "        best_params = grid_search[\"model\"].best_params_\n",
    "        bin_ = grid_search[\"bin\"]\n",
    "        img_size_ = grid_search[\"img_size\"]\n",
    "\n",
    "\n",
    "# In kết quả\n",
    "print(\"\\nBest Overall Model:\")\n",
    "print(\"img_size:\", img_size_)\n",
    "print(\"bin:\", bin_)\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# save parameters\n",
    "params = {\n",
    "    \"img_size\": img_size_,\n",
    "    \"bin\": bin_,\n",
    "}\n",
    "params.update(best_params)\n",
    "print(params)\n",
    "\n",
    "with open('./checkpoint/svm_params.json', 'w') as f:\n",
    "    json.dump(params, f)\n",
    "\n",
    "#save model\n",
    "joblib.dump(best_model, './checkpoint/svm_best_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *KNN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n",
      "Eplased time: 264.8366196155548\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bins = [8, 16, 32, 64, 128]\n",
    "sizes = [64, 128, 250, 512]  # default image_size = 250x250 \n",
    "\n",
    "results = []\n",
    "start = time.time()\n",
    "for bin in bins:\n",
    "    for img_size in sizes:\n",
    "        knn = KNeighborsClassifier()\n",
    "        train_data_ = [preprocess2(x, img_size, bin) for x in train_data]\n",
    "\n",
    "        param_grid = {\n",
    "            'n_neighbors': [1, 3, 5, 7, 9],         \n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'metric': [\n",
    "                'euclidean', 'manhattan', 'minkowski', 'chebyshev', 'cosine', 'hamming'\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # GridSearchCV\n",
    "        grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "        # Huấn luyện\n",
    "        grid_search.fit(train_data_, train_labels)\n",
    "        results.append({\n",
    "            \"bin\": bin,\n",
    "            \"img_size\": img_size,\n",
    "            \"model\": grid_search           \n",
    "        }\n",
    "        )\n",
    "\n",
    "print(\"DONE\")\n",
    "elapsed = time.time() - start\n",
    "print(f\"Eplased time: {elapsed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Overall Model:\n",
      "img_size: 250\n",
      "bin: 16\n",
      "Best Parameters: {'metric': 'manhattan', 'n_neighbors': 7, 'weights': 'distance'}\n",
      "{'img_size': 250, 'bin': 16, 'metric': 'manhattan', 'n_neighbors': 7, 'weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_score = -float('inf')\n",
    "\n",
    "for grid_search in results:\n",
    "    if grid_search[\"model\"].best_score_ > best_score:\n",
    "        best_score = grid_search[\"model\"].best_score_\n",
    "        best_model = grid_search[\"model\"].best_estimator_\n",
    "        best_params = grid_search[\"model\"].best_params_\n",
    "        bin_ = grid_search[\"bin\"]\n",
    "        img_size_ = grid_search[\"img_size\"]\n",
    "\n",
    "\n",
    "print(\"\\nBest Overall Model:\")\n",
    "print(\"img_size:\", img_size_)\n",
    "print(\"bin:\", bin_)\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "params = {\n",
    "    \"img_size\": img_size_,\n",
    "    \"bin\": bin_,\n",
    "}\n",
    "params.update(best_params)\n",
    "print(params)\n",
    "\n",
    "# #save params\n",
    "# with open('./checkpoint/knn_params.json', 'w') as f:\n",
    "#     json.dump(params, f)\n",
    "\n",
    "# # save model \n",
    "# joblib.dump(best_model, './checkpoint/knn_best_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Random Forest*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bins:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bins: 100%|██████████| 6/6 [11:17<00:00, 112.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n",
      "Total Time Taken: 677.16 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "results = []\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "bins = [8, 16, 32, 64, 128, 256]\n",
    "sizes = [32, 64, 128, 250]  # default image_size = 250x250 \n",
    "\n",
    "for bin in tqdm(bins, desc=\"Bins\"):\n",
    "    for img_size in tqdm(sizes, desc=\"Image Sizes\", leave=False):\n",
    "        rf = RandomForestClassifier(random_state=42, n_jobs=-1)  # Use all CPU cores\n",
    "\n",
    "        train_data_ = [preprocess2(x, img_size, bin) for x in train_data]\n",
    "\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "        }\n",
    "\n",
    "        # GridSearchCV\n",
    "        grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "        # Train\n",
    "        grid_search.fit(train_data_, train_labels)\n",
    "        results.append({\n",
    "            \"bin\": bin,\n",
    "            \"img_size\": img_size,\n",
    "            \"model\": grid_search           \n",
    "        })\n",
    "\n",
    "# End timer\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(\"DONE\")\n",
    "print(f\"Total Time Taken: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# # Save the results list to a file for future use\n",
    "# with open('./checkpoint/results.pkl', 'wb') as f:\n",
    "#     joblib.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Overall Model:\n",
      "img_size: 128\n",
      "bin: 8\n",
      "Best Parameters: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "{'img_size': 128, 'bin': 8, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_score = -float('inf')\n",
    "\n",
    "for grid_search in results:\n",
    "    if grid_search[\"model\"].best_score_ > best_score:\n",
    "        best_score = grid_search[\"model\"].best_score_\n",
    "        best_model = grid_search[\"model\"].best_estimator_\n",
    "        best_params = grid_search[\"model\"].best_params_\n",
    "        bin_ = grid_search[\"bin\"]\n",
    "        img_size_ = grid_search[\"img_size\"]\n",
    "\n",
    "\n",
    "print(\"\\nBest Overall Model:\")\n",
    "print(\"img_size:\", img_size_)\n",
    "print(\"bin:\", bin_)\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "params = {\n",
    "    \"img_size\": img_size_,\n",
    "    \"bin\": bin_,\n",
    "}\n",
    "params.update(best_params)\n",
    "print(params)\n",
    "\n",
    "# #save params\n",
    "# with open('./checkpoint/rf_params.json', 'w') as f:\n",
    "#     json.dump(params, f)\n",
    "\n",
    "# # save model \n",
    "# joblib.dump(best_model, './checkpoint/rf_best_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
